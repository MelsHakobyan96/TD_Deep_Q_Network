# TD_Deep_Q_Network

 Deep Q Network using TD learning, experience replay and also I implemented "real" Boltzman exploration method which led to some great results. Code is based on famous code from openai website, but I hoped the class architecture would be more flexible than my previous class, but I was wrong. 
 Experience replay helped in learning for lunar-lander but I want to implement Monte-Carlo technique with experience replay as well, hopefuly it will help in reducing learning variance and it will converge much faster, also thinking about implementing Dualing DQN, and Double DQN later.
